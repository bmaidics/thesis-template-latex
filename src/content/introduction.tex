%----------------------------------------------------------------------------
\chapter{\bevezetes}
%----------------------------------------------------------------------------

The digital era has led to large amounts of data being amassed by companies every day. Data comes from multiple sources: sensors, sales data, communication systems, logging of system events, \etc. According to Forbes \cite{Forbes}, every day 2.5 quintillion bytes – 2.5 million Terabytes – of data are created. Larger corporations can easily create hundreds of Terabytes daily. We need a new solution to process this amount of data. The traditional relational databases (RDBMS) can deal only with Gigabytes. Hadoop provides a software framework to scale up our system for storing, processing and analyzing big data.

Hadoop is the most popular implementation of the map-reduce programming paradigm and widely used to process and store extremely large datasets in a distributed way. However, a map-reduce program is very low level and difficult to maintain or reuse. Data scientists come from a world where SQL is the standard of querying data. Apache Hive gives us a data warehouse solution built on top of Hadoop to write SQL-like queries so we can utilize the advantages of a declarative language.

In my thesis, I will present the basics of Apache Hadoop and its different subcomponents, like HDFS, Hadoop MapReduce and YARN. I will also show how Apache Hive works on top of Hadoop and its architecture. However, Hive faces multiple memory issues and limitations which should be investigated and if possible reduce memory consumption.

The main part of the thesis will present the method  I developed to measure the memory of Hive in different phases of the compilation and execution. Using this, my task will be to get a better understanding of what factors cause huge memory increases and when it happens during the lifecycle of a query. I will present two issues that I found with the help of my measuring tool and other analysis tools. One of these comes from the HDFS dependency of Hive \cite{hdfs-path}, so further investigation of Hadoop and especially HDFS was also needed. 


Basically, both memory overheads were caused by duplicate objects. In HDFS, a built-in URI class caused the issue. It is likely that the memory of these objects are negligible in a smaller application, but in a service where we can have millions of those, they can put notable pressure on the heap size. Here, my suggestion was to unwrap the fields inside the class that caused the problem. These resulted in around 60\% of decrease per objects so when having thousands of those might be a big help.

Apache Hive also has some overhead introduced by multiple configuration objects with nearly identical content. My idea as a solution was to divide the objects that cause overhead into two parts, and intern the duplicated values. With my change, each object uses 40\% less memory. \cite{hive-conf}.